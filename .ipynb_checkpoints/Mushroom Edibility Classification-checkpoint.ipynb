{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**In this project, we'll be using machine learning techniques to classify samples of mushrooms as either edible or poisonous.** The data set for training and testing was found on [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. The latter class was combined with the poisonous one. You can download the data set and access its documentation [here](https://archive.ics.uci.edu/ml/datasets/mushroom).\n",
    "\n",
    "![mushrooms](mushrooms.png)\n",
    "\n",
    "The various columns and their descriptions are below:\n",
    "\n",
    "| column | description |\n",
    "| :---| :--- |\n",
    "| `edibility` | `e` = edible, `p` = poisonous |\n",
    "| `cap_shape` | `b` = bell, `c` = conical, `f` = flat, `k` = knobbed, `s` = sunken, `x` = convex |\n",
    "| `cap_surface` | `f` = fibrous, `g` = grooves, `s` = smooth, `y` = scaly |\n",
    "| `cap_color` | `b` = buff, `c` = cinnamon, `e` = red, `g` = gray, `n` = brown, `p` = pink, `r` = green, `u` = purple, `w` = white, `y` = yellow |\n",
    "| `bruises` | `f` = no, `t` = bruises |\n",
    "| `odor` | `a` = almond, `c` = creosote, `f` = foul, `l` = anise, `m` = musty, `n` = none ,`p` = pungent, `s` = spicy, `y` = fishy |\n",
    "| `gill_attachment` | `a` = attached, `d` = descending, `f` = free, `n` = notched |\n",
    "| `gill_spacing` | `c` = close, `d` = distant, `w` = crowded |\n",
    "| `gill_size` | `b` = broad, `n` = narrow |\n",
    "| `gill_color` | `b` = buff, `e` = red, `g` = gray, `h` = chocolate, `k` = black, `n` = brown, `o` = orange, `p` = pink, `r` = green, `u` = purple, `w` = white, `y` = yellow |\n",
    "| `stalk_shape` | `e` = enlarging, `t` = tapering |\n",
    "| `stalk_root` | `b` = bulbous, `c` = club, `e` = equal, `r` = rooted, `u` = cup, `z` = rhizomorph, `?` = missing |\n",
    "| `stalk_surface_above_ring` | `f` = fibrous, `k` = silky, `s` = smooth, `y` = scaly |\n",
    "| `stalk_surface_below_ring` | `f` = fibrous, `k` = silky, `s` = smooth, `y` = scaly |\n",
    "| `stalk-color-above-ring` | `b` = buff, `c` = cinnamon, `e` = red, `g` = gray, `n` = brown, `o` = orange, `p` = pink, `w` = white, `y` = yellow |\n",
    "| `stalk-color-below-ring` | `b` = buff, `c` = cinnamon, `e` = red, `g` = gray, `n` = brown, `o` = orange, `p` = pink, `w` = white, `y` = yellow |\n",
    "| `veil_type` | `p` = partial, `u` = universal |\n",
    "| `veil_color` | `n` = brown, `o` = orange, `w` = white, `y` = yellow |\n",
    "| `ring_number` | `n` = none, `o` = one, `t` = two |\n",
    "| `ring_type` | `c` = cobwebby, `e` = evanescent, `f` = flaring, `l` = large, `n` = none, `p` = pendant, `s` = sheathing, `z` = zone |\n",
    "| `spore_print_color` | `b` = buff, `h` = chocolate, `k` = black, `n` = brown, `o` = orange, `r` = green, `u` = purple, `w` = white, `y` = yellow |\n",
    "| `population` | `a` = abundant, `c` = clustered, `n` = numerous, `s` = scattered, `v` = several, `y` = solitary |\n",
    "| `habitat` | `d` = woods, `g` = grasses, `l` = leaves, `m` = meadows, `p` = paths, `u` = urban, `w` = waste |\n",
    "\n",
    "Let's read in the data set and look at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File agaricus-lepiota.data does not exist: 'agaricus-lepiota.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bf38e1f753fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m           'ring_number', 'ring_type', 'spore_print_color', 'population', 'habitat']\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agaricus-lepiota.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File agaricus-lepiota.data does not exist: 'agaricus-lepiota.data'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "header = ['edibility', 'cap_shape', 'cap_surface', 'cap_color', 'bruises', 'odor', 'gill_attachment',\n",
    "          'gill_spacing', 'gill_size', 'gill_color', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring',\n",
    "          'stalk_surface_below_ring', 'stalk_color_above_ring', 'stalk_color_below_ring', 'veil_type', 'veil_color',\n",
    "          'ring_number', 'ring_type', 'spore_print_color', 'population', 'habitat']\n",
    "\n",
    "mu = pd.read_csv('agaricus-lepiota.data', names=header)\n",
    "mu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we have 8,124 rows and 23 columns. The target column containing the class we aim to predict is `edibility` while the remaining 22 columns are the features which will aid us in the process.\n",
    "\n",
    "Do we have a balanced class label distribution for our target column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.edibility.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are very close to a 50/50 split between edible and poisonous, which means we need not worry about issues related to skewed class label distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Do we have any null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There do not appear to be any null values. However, according to the data set documentation, there are 2,480 missing values denoted by `?` all within `stalk_root`. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.stalk_root.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There they are! Let's replace the question marks with `np.NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.stalk_root = mu.stalk_root.replace('?', np.NaN)\n",
    "mu.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that 30% of the values in `stalk_root` are missing, we'll simply drop the entire column from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mu.dropna(axis=1)\n",
    "mu.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 8,124 rows and **22** columns including our target. Before applying machine learning algorithms, we must transform the categorical data into a numerical form suitable for analysis. Creating [dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) for each feature variable will accomplish our goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mu.columns.drop('edibility')\n",
    "# create `mu_dum` -- dummy variable version of `mu`\n",
    "mu_dum = pd.concat([pd.get_dummies(mu[features]), mu.edibility], axis=1)\n",
    "mu_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_dum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our feature columns have been expanded so that each category of each feature has its own column indicating either its presence or absence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors algorithm\n",
    "\n",
    "Let's train some simple models using the k-nearest neighbors (KNN) algorithm and see how they fare. Since we are dealing with categorical variables -- not continuous nor ordinal -- our distance metric will be [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) instead of the more usual Euclidean distance.\n",
    "\n",
    "## Fine-tuning `k`\n",
    "\n",
    "Before fine-tuning which features to consider in our model, we will arbitrarily restrict ourselves to \"cap characteristics\" (`cap_shape`, `cap_surface`, `cap_color`) and fine-tune `k`, the number of neighbors used to calculate predictions. Specifically, we will train multiple versions of three KNN models, one univariate, one bivariate, and one multivariate. For each type, we will range the value of `k` to see its effect on model accuracy.\n",
    "\n",
    "### Univariate model: `cap_shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 15, 17] # odd values to simplify classification by majority vote\n",
    "mean_accuracies = [] # list to store mean accuracy for each k value\n",
    "\n",
    "features = list(mu_dum.columns[mu_dum.columns.str.startswith('cap_shape')])\n",
    "target = 'edibility'\n",
    "\n",
    "for k in k_vals:\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # calculate and store mean accuracy\n",
    "    mean_accuracies.append(accuracy_values.mean())\n",
    "    \n",
    "uni_k_df = pd.DataFrame({'k_value': k_vals, 'mean_accuracy': mean_accuracies})\n",
    "uni_k_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate model: `cap_shape`, `cap_surface`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 15, 17] # odd values to simplify classification by majority vote\n",
    "mean_accuracies = [] # list to store mean accuracy for each k value\n",
    "\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('cap_shape')) |\n",
    "                               (mu_dum.columns.str.startswith('cap_surface'))])\n",
    "target = 'edibility'\n",
    "\n",
    "for k in k_vals:\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # calculate and store mean accuracy\n",
    "    mean_accuracies.append(accuracy_values.mean())\n",
    "    \n",
    "bi_k_df = pd.DataFrame({'k_value': k_vals, 'mean_accuracy': mean_accuracies})\n",
    "bi_k_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate model: `cap_shape`, `cap_surface`, `cap_color`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 15, 17] # odd values to simplify classification by majority vote\n",
    "mean_accuracies = [] # list to store mean accuracy for each k value\n",
    "\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('cap_shape')) |\n",
    "                               (mu_dum.columns.str.startswith('cap_surface')) |\n",
    "                               (mu_dum.columns.str.startswith('cap_color'))])\n",
    "target = 'edibility'\n",
    "\n",
    "for k in k_vals:\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # calculate and store mean accuracy\n",
    "    mean_accuracies.append(accuracy_values.mean())\n",
    "    \n",
    "multi_k_df = pd.DataFrame({'k_value': k_vals, 'mean_accuracy': mean_accuracies})\n",
    "multi_k_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot mean accuracies against k-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.title('Mean accuracy vs. k-value')\n",
    "\n",
    "c_gray = (89/255,89/255,89/255)\n",
    "c_blue = (0/255,107/255,164/255)\n",
    "c_orange = (255/255,128/255,14/255)\n",
    "\n",
    "plt.plot(uni_k_df['k_value'], uni_k_df['mean_accuracy'], c=c_gray, label='univariate')\n",
    "plt.text(13, 0.525, 'univariate', {'c':c_gray})\n",
    "\n",
    "plt.plot(bi_k_df['k_value'], bi_k_df['mean_accuracy'], c=c_blue, label='bivariate')\n",
    "plt.text(13, 0.605, 'bivariate', {'c':c_blue})\n",
    "\n",
    "plt.plot(multi_k_df['k_value'], multi_k_df['mean_accuracy'], c=c_orange, label='multivariate')\n",
    "plt.text(13, 0.675, 'multivariate', {'c':c_orange})\n",
    "\n",
    "plt.xticks(k_vals)\n",
    "plt.xlabel('k_value')\n",
    "plt.ylabel('mean_accuracy')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the univariate case, we see mostly moderate increases in accuracy as we move from `k = 1` to `k = 13`. For the bivariate case, we see a sharp increase from `k = 3` to `k = 5` followed by moderate increases up to `k = 15`. The multivariate case is similar with a sharp jump up from `k = 3` to `k = 5`, but the accuracy seems to hover around `0.68` no matter how great we increase `k`.\n",
    "\n",
    "Since the multivariate model produced the greatest accuracy, let's test additional multivariate models to see if `k = 5` is indeed ideal. We'll simply select a random set of three features and repeat the process to see how `k` affects accuracy of the model.\n",
    "\n",
    "### More multivariate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second multivariate model using `gill_spacing`, `stalk_color_below_ring`, `population`\n",
    "\n",
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 15, 17] # odd values to simplify classification by majority vote\n",
    "mean_accuracies = [] # list to store mean accuracy for each k value\n",
    "\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('gill_spacing')) |\n",
    "                               (mu_dum.columns.str.startswith('stalk_color_below_ring')) |\n",
    "                               (mu_dum.columns.str.startswith('population'))])\n",
    "target = 'edibility'\n",
    "\n",
    "for k in k_vals:\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # calculate and store mean accuracy\n",
    "    mean_accuracies.append(accuracy_values.mean())\n",
    "    \n",
    "multi_k_df_2 = pd.DataFrame({'k_value': k_vals, 'mean_accuracy': mean_accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third multivariate model using `cap_surface`, `veil_type`, `bruises`\n",
    "\n",
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 15, 17] # odd values to simplify classification by majority vote\n",
    "mean_accuracies = [] # list to store mean accuracy for each k value\n",
    "\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('cap_surface')) |\n",
    "                               (mu_dum.columns.str.startswith('veil_type')) |\n",
    "                               (mu_dum.columns.str.startswith('bruises'))])\n",
    "target = 'edibility'\n",
    "\n",
    "for k in k_vals:\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # calculate and store mean accuracy\n",
    "    mean_accuracies.append(accuracy_values.mean())\n",
    "    \n",
    "multi_k_df_3 = pd.DataFrame({'k_value': k_vals, 'mean_accuracy': mean_accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fourth multivariate model using `veil_color`, `ring_number`, `ring_type`\n",
    "\n",
    "k_vals = [1, 3, 5, 7, 9, 11, 13, 15, 17] # odd values to simplify classification by majority vote\n",
    "mean_accuracies = [] # list to store mean accuracy for each k value\n",
    "\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('veil_color')) |\n",
    "                               (mu_dum.columns.str.startswith('ring_number')) |\n",
    "                               (mu_dum.columns.str.startswith('ring_type'))])\n",
    "target = 'edibility'\n",
    "\n",
    "for k in k_vals:\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # calculate and store mean accuracy\n",
    "    mean_accuracies.append(accuracy_values.mean())\n",
    "    \n",
    "multi_k_df_4 = pd.DataFrame({'k_value': k_vals, 'mean_accuracy': mean_accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all four multivariate models' accuracies vs. k-values\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.title('Mean accuracy vs. k-value')\n",
    "\n",
    "c_gray = (89/255,89/255,89/255)\n",
    "c_blue = (0/255,107/255,164/255)\n",
    "c_orange = (255/255,128/255,14/255)\n",
    "c_red = (200/255,82/255,0/255)\n",
    "\n",
    "plt.plot(multi_k_df['k_value'], multi_k_df['mean_accuracy'], c=c_gray, label='one')\n",
    "\n",
    "plt.plot(multi_k_df_2['k_value'], multi_k_df_2['mean_accuracy'], c=c_blue, label='two')\n",
    "\n",
    "plt.plot(multi_k_df_3['k_value'], multi_k_df_3['mean_accuracy'], c=c_orange, label='three')\n",
    "\n",
    "plt.plot(multi_k_df_4['k_value'], multi_k_df_4['mean_accuracy'], c=c_red, label='four')\n",
    "\n",
    "plt.xticks(k_vals)\n",
    "plt.xlabel('k_value')\n",
    "plt.ylabel('mean_accuracy')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, most models' accuracies plateau by `k = 5`. The fourth plot in red actually decreases as `k` increases to `7` and `9`. Considering all of the above, we will move forward with `k = 5` as our main benchmark. Now that we've fine-tuned `k`, let's turn our attention to feature selection.\n",
    "\n",
    "## Feature selection\n",
    "\n",
    "Being more of an expert in machine learning rather than mushroom classification, we'll leverage \"feature variability\" to make an educated selection. This is similar to the concept of variance but applied to categorical data. Details of the technique can be found [here](http://jse.amstat.org/v15n2/kader.pdf).\n",
    "\n",
    "### Feature variability\n",
    "\n",
    "Basically, for a categorical variable with $n$ categories, its variability can be defined as:\n",
    "\n",
    "$1 - p_1^2 - p_2^2 - p_3^2 - \\cdots - p_n^2$,\n",
    "\n",
    "where $p_i$ is the proportion of all values in the $i$'th category. Values closer to `0` indicate low variability while values closer to `1` indicate high variability. Let's calculate the variability for a couple of columns to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `cap_shape` for example, we obtain the proportions of each category with the `value_counts()` method\n",
    "# set `normalize=True` to use proportions rather than counts\n",
    "cat_ps = mu.cap_shape.value_counts(normalize=True)\n",
    "cat_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we simply square each proportion and subtract their total from 1\n",
    "cap_shape_variability = 1 - (cat_ps**2).sum()\n",
    "cap_shape_variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `cap_shape`, a variable with values mostly consisting of `x` and `f` and smaller amounts distributed between `k`, `b`, `s`, and `c`, we calculated a variability of about `0.63`, which seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more example with `veil_type`\n",
    "cat_ps = mu.veil_type.value_counts(normalize=True)\n",
    "cat_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veil_type_variability = 1 - (cat_ps**2).sum()\n",
    "veil_type_variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for `veil_type`, a variable with values **entirely** consisting of `p`, we calculated a variability of exactly `0`. Perfect!\n",
    "\n",
    "Now, we will move on and calculate the variability of each feature. In the context of feature selection, we will choose features with higher variability since features with lower variability tend to have lower explanatory power. As an illustration, imagine using only `veil_type` as a feature. All neighbors would be equidistant and classification would essentially be random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {}\n",
    "\n",
    "for col in mu.columns.drop('edibility'):\n",
    "    var = 1 - (mu[col].value_counts(normalize=True)**2).sum()\n",
    "    var_dict[col] = var\n",
    "    \n",
    "var_df = pd.DataFrame.from_dict(var_dict, orient='index', columns=['variability'])\n",
    "var_df = var_df.sort_values('variability', ascending=False)\n",
    "var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High variability features\n",
    "\n",
    "Let's take a look at features with a variability greater than `0.67`. We'll train and test one univariate model for each feature and store the results to see how well each feature performs on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_var_feats = list(var_df[var_df.variability > 0.67].index)\n",
    "high_var_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store mean accuracy for each individual feature\n",
    "one_feat_acc = {}\n",
    "\n",
    "for feat in high_var_feats:\n",
    "    features = list(mu_dum.columns[mu_dum.columns.str.startswith(feat)])\n",
    "    target = 'edibility'\n",
    "\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "\n",
    "    # store results\n",
    "    one_feat_acc[feat] = accuracy_values.mean()\n",
    "    \n",
    "# convert dictionary to dataframe, sort and display results\n",
    "one_feat_acc = pd.DataFrame.from_dict(one_feat_acc, orient='index', columns=['accuracy'])\n",
    "one_feat_acc = one_feat_acc.sort_values('accuracy', ascending=False)\n",
    "one_feat_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that `odor` by itself yielded an accuracy of `98.5%`! Can we improve with a bivariate model that uses `odor` along with another feature? Let's explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations with `odor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store second feature and mean accuracy\n",
    "odor_d = {}\n",
    "\n",
    "for feat in list(mu.columns.drop(['edibility', 'odor'])):\n",
    "    features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                                   (mu_dum.columns.str.startswith(feat))])\n",
    "    target = 'edibility'\n",
    "\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # store results\n",
    "    odor_d[feat] = accuracy_values.mean()\n",
    "    \n",
    "# convert dictionary to dataframe, sort and display results\n",
    "odor_df = pd.DataFrame.from_dict(odor_d, orient='index', columns=['accuracy'])\n",
    "odor_df = odor_df.sort_values('accuracy', ascending=False)\n",
    "# only display features which significantly outperformed univariate model with `odor` alone\n",
    "odor_df[odor_df.accuracy > 0.986]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better! We now have many models with an accuracy of at least `98.6%`. Using `odor` and `spore_print_color` actually produced an accuracy of `99.4%`! Let's see if we can continue to improve by keeping `odor` and `spore_print_color` and adding a third feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations with `odor`, `spore_print_color`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store third feature and mean accuracy\n",
    "o_spc_d = {}\n",
    "\n",
    "for feat in list(mu.columns.drop(['edibility', 'odor', 'spore_print_color'])):\n",
    "    features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                                   (mu_dum.columns.str.startswith('spore_print_color')) |\n",
    "                                   (mu_dum.columns.str.startswith(feat))])\n",
    "    target = 'edibility'\n",
    "\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # store results\n",
    "    o_spc_d[feat] = accuracy_values.mean()\n",
    "    \n",
    "# convert dictionary to dataframe, sort and display results\n",
    "o_spc_df = pd.DataFrame.from_dict(o_spc_d, orient='index', columns=['accuracy'])\n",
    "o_spc_df = o_spc_df.sort_values('accuracy', ascending=False)\n",
    "# only display features which significantly outperformed bivariate model with `odor` and `spore_print_color`\n",
    "o_spc_df[o_spc_df.accuracy > 0.995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more slight improvements! The multivariate model using `odor`, `spore_print_color`, and `cap_color` yielded an accuracy of `99.7%`. Let's continue iterating by adding `cap_color` into the mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations with `odor`, `spore_print_color`, `cap_color`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store fourth feature and mean accuracy\n",
    "o_spc_cc_d = {}\n",
    "\n",
    "for feat in list(mu.columns.drop(['edibility', 'odor', 'spore_print_color', 'cap_color'])):\n",
    "    features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                                   (mu_dum.columns.str.startswith('spore_print_color')) |\n",
    "                                   (mu_dum.columns.str.startswith('cap_color')) |\n",
    "                                   (mu_dum.columns.str.startswith(feat))])\n",
    "    target = 'edibility'\n",
    "\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # store results\n",
    "    o_spc_cc_d[feat] = accuracy_values.mean()\n",
    "    \n",
    "# convert dictionary to dataframe, sort and display results\n",
    "o_spc_cc_df = pd.DataFrame.from_dict(o_spc_cc_d, orient='index', columns=['accuracy'])\n",
    "o_spc_cc_df = o_spc_cc_df.sort_values('accuracy', ascending=False)\n",
    "# display features which outperformed multivariate model with `odor`, `spore_print_color`, `cap_color`\n",
    "o_spc_cc_df[o_spc_cc_df.accuracy > 0.998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate models with `odor`, `spore_print_color`, `cap_color`, and either `stalk_surface_above_ring` or `habitat` both produced accuracies of `99.9%`! At this point, it would be reasonable to end the feature selection process, but the possibility of `100%` accuracy is too enticing to pass up.\n",
    "\n",
    "Before moving forward and searching for a fourth and fifth feature, let's explore whether a different combination of three features may prove more fruitful. To that end, we will train and test a model for each possible combination of three features among the features with high variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All combinations of three among high variability features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# generate all possible combinations of three among high variability features\n",
    "combos = combinations(high_var_feats, 3)\n",
    "# dictionary to store combination with mean accuracy\n",
    "combo_acc_d = {}\n",
    "\n",
    "for combo in combos:\n",
    "    feat1, feat2, feat3 = combo\n",
    "    features = list(mu_dum.columns[(mu_dum.columns.str.startswith(feat1)) |\n",
    "                                   (mu_dum.columns.str.startswith(feat2)) |\n",
    "                                   (mu_dum.columns.str.startswith(feat3))])\n",
    "    target = 'edibility'\n",
    "\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "\n",
    "    # store results\n",
    "    combo_acc_d[combo] = accuracy_values.mean()\n",
    "    \n",
    "# convert dictionary to dataframe, sort and display results\n",
    "combo_acc_df = pd.DataFrame.from_dict(combo_acc_d, orient='index', columns=['accuracy'])\n",
    "combo_acc_df = combo_acc_df.sort_values('accuracy', ascending=False)\n",
    "# only display features which significantly outperformed bivariate model with `odor` and `spore_print_color`\n",
    "combo_acc_df[combo_acc_df.accuracy > 0.995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all the features in the top-performing combinations above, we see the usual suspects: `odor`, `spore_print_color`, and `cap_color` but also `habitat` and `population`. Since `cap_color` was the latest addition to our model, let's keep `odor` and `spore_print_color` but replace `cap_color` with `habitat` and see if accuracy improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations with `odor`, `spore_print_color`, `habitat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store fourth feature and mean accuracy\n",
    "o_spc_h_d = {}\n",
    "\n",
    "for feat in list(mu.columns.drop(['edibility', 'odor', 'spore_print_color', 'habitat'])):\n",
    "    features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                                   (mu_dum.columns.str.startswith('spore_print_color')) |\n",
    "                                   (mu_dum.columns.str.startswith('habitat')) |\n",
    "                                   (mu_dum.columns.str.startswith(feat))])\n",
    "    target = 'edibility'\n",
    "\n",
    "    # instantiate model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "    # instantiate k-folds for cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    # train and test model\n",
    "    accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "    \n",
    "    # store results\n",
    "    o_spc_h_d[feat] = accuracy_values.mean()\n",
    "\n",
    "# convert dictionary to dataframe, sort and display results\n",
    "o_spc_h_df = pd.DataFrame.from_dict(o_spc_h_d, orient='index', columns=['accuracy'])\n",
    "o_spc_h_df = o_spc_h_df.sort_values('accuracy', ascending=False)\n",
    "# display features which outperformed multivariate model with `odor`, `spore_print_color`, `cap_color`\n",
    "o_spc_h_df[o_spc_h_df.accuracy > 0.998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is! It seems the magic combination is `odor`, `spore_print_color`, `habitat`, and **`population`**.\n",
    "\n",
    "Let's dig in further to confirm. We had been using 5-fold cross-validation, but let's increase to 10-fold cross-validation for more confidence in our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 4: `odor`, `spore_print_color`, `habitat`, `population`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                               (mu_dum.columns.str.startswith('spore_print_color')) |\n",
    "                               (mu_dum.columns.str.startswith('habitat')) |\n",
    "                               (mu_dum.columns.str.startswith('population'))])\n",
    "target = 'edibility'\n",
    "\n",
    "# instantiate model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "# instantiate k-folds for cross-validation, set `n_splits=10`\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# train and test model\n",
    "accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "\n",
    "# display results\n",
    "for i in range(len(accuracy_values)):\n",
    "    print('fold', i+1, 'accuracy:', accuracy_values[i])\n",
    "print('\\nmean accuracy:', accuracy_values.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we got lucky with the splitting of train and test sets. Let's vary the `random_state` parameter of the `KFold` instance to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same model with different train/test split\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                               (mu_dum.columns.str.startswith('spore_print_color')) |\n",
    "                               (mu_dum.columns.str.startswith('habitat')) |\n",
    "                               (mu_dum.columns.str.startswith('population'))])\n",
    "target = 'edibility'\n",
    "\n",
    "# instantiate model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "# instantiate k-folds for cross-validation, set `n_splits=10`\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=2) # set `random_state=2`\n",
    "\n",
    "# train and test model\n",
    "accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "\n",
    "# display results\n",
    "for i in range(len(accuracy_values)):\n",
    "    print('fold', i+1, 'accuracy:', accuracy_values[i])\n",
    "print('\\nmean accuracy:', accuracy_values.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same model again with different train/test split\n",
    "features = list(mu_dum.columns[(mu_dum.columns.str.startswith('odor')) |\n",
    "                               (mu_dum.columns.str.startswith('spore_print_color')) |\n",
    "                               (mu_dum.columns.str.startswith('habitat')) |\n",
    "                               (mu_dum.columns.str.startswith('population'))])\n",
    "target = 'edibility'\n",
    "\n",
    "# instantiate model\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='brute', metric='hamming')\n",
    "# instantiate k-folds for cross-validation, set `n_splits=10`\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=3) # set `random_state=3`\n",
    "\n",
    "# train and test model\n",
    "accuracy_values = cross_val_score(knn, mu_dum[features], mu_dum[target], cv=kf)\n",
    "\n",
    "# display results\n",
    "for i in range(len(accuracy_values)):\n",
    "    print('fold', i+1, 'accuracy:', accuracy_values[i])\n",
    "print('\\nmean accuracy:', accuracy_values.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twice more, we receive `100%` accuracy, so our initial findings were not just a fluke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using the k-nearest neighbors algorithm, we achieved `100%` accuracy using the following 4 features:\n",
    "\n",
    "- `odor`\n",
    "- `spore_print_color`\n",
    "- `habitat`\n",
    "- `population`\n",
    "\n",
    "Parameters for the model were as follows:\n",
    "\n",
    "- `n_neighbors=5`\n",
    "- `weights='uniform'`\n",
    "- `algorithm='brute'`\n",
    "- `metric='hamming'`\n",
    "\n",
    "This was simply the highest-performing KNN model with the least features that I was personally able to find. By no means was my process exhaustive. Equally as accurate, simpler KNN models very well might exist. However, if presented with brand new data describing similar types of mushrooms (those in the Agaricus and Lepiota Families), we can be reasonably confident that our model will classify edible and poisonous samples correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
